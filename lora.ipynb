{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, rank, hidden_size_in, hidden_size_out, alpha):\n",
    "        super().__init__()\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = nn.Parameter(torch.randn(rank, hidden_size_out))\n",
    "        self.B = nn.Parameter(torch.zeros(hidden_size_in, rank))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.B @ self.A @ x\n",
    "        \n",
    "        return x * self.alpha * 1 / self.std_dev\n",
    "\n",
    "\n",
    "\n",
    "class LoRADecorator(nn.Module):\n",
    "    def __init__(self, module, rank, alpha):\n",
    "        super().__init__()\n",
    "\n",
    "        self.module = module\n",
    "        self.weight = module.weight\n",
    "        self.bias = module.bias\n",
    "\n",
    "        self.lora = LoRALayer(rank, module.in_features, module.out_features, alpha)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_module = self.module(x)\n",
    "        x_lora = self.lora(x)\n",
    "        return x_module + x_lora \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = nn.Linear(128, 128)\n",
    "lora = LoRADecorator(rank=3, module=mod, alpha = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "lora(torch.randn(128)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # lets setup a quick training so we see matrixes changing\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# lora = LoRALayer(rank=3, hidden_size_in=128, hidden_size_out=128, alpha = 10)\n",
    "# optimizer = optim.Adam(lora.parameters(), lr=0.01)\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# for i in range(1000):\n",
    "#     optimizer.zero_grad()\n",
    "#     x = torch.randn(128)\n",
    "#     y = lora(x)\n",
    "#     loss = criterion(x, y)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "# print(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import TransformerDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerDecoder(nn.TransformerDecoderLayer(512, 8), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traverse the model, print all the trainable layer names\n",
    "# do it recursively\n",
    "from functools import reduce  \n",
    "\n",
    "\n",
    "lora_to_replace = ['linear', 'out_proj']\n",
    "\n",
    "def apply_lora(model, rank, alpha):  \n",
    "    for name, layer in model.named_modules():  \n",
    "        if len(list(layer.children())) == 0:  \n",
    "            name_child = name.split('.')[-1] \n",
    "            if name_child[-1].isdigit():\n",
    "                name_child = name_child[:-1]\n",
    "            if name_child  in lora_to_replace and not isinstance(layer, LoRALayer):\n",
    "                new_layer = LoRADecorator(layer, rank, alpha)\n",
    "                # get the parent module  \n",
    "                parent_name, child_name = name.rsplit('.', 1)  \n",
    "                parent_module = reduce(getattr, parent_name.split('.'), model)  \n",
    "                # replace the layer in the parent module  \n",
    "                parent_module._modules[child_name] = new_layer  \n",
    "    return model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = apply_lora(model, 3, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.randn(10, 32, 512), memory=torch.randn(10, 32, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(lora.lora.B @ lora.lora.A @ torch.randn(128)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
